Distributor — это сервис, который равномерно распределяет единицы работы между сервисами. 
Если у вас есть некоторое множество сервисов, между которыми надо распределить ответственность, Distributor позволит не делать это вручную. 
При этом Distributor позволяет делать такое распределение детерминированно: для одного и того же списка сервисов и единиц работы он выдает одинаковое распределение независимо от прочих условий (времени, инстанса самого Distributor и т.д.).

Например, вам нужно парсить определенное множество веб-сайтов. У вас есть некоторый парсер, который запущен на нескольких машинах. 
Каждый инстанс парсера должен парсить свою подгруппу сайтов. Пересечений быть не должно: если один парсер с ID "pX" уже парсит сайт "sX", никакой другой парсер не должен выполнять ту же работу повторно. 
Кроме того, каждый парсер должен получить одну и ту же группу веб-сайтов, даже если его запустят спустя N дней на новой машине. Distributor назначит каждому парсеру свою подгруппу сайтов в соответствии со всеми этими требованиями. 
Если один парсер упадет или количество веб-сайтов поменяется — работа будет перераспределена.

Distributor можно запускать на разных машинах для обеспечения отказоустойчивости.

<br>

#### Хранилище

Distributor с дефолтной имплементацией хранилища должен запускаться с Redis (кластер или одиночный инстанс). 
Redis хранит список сервисов по опредленному ключу (список таких ключей указывается через запятую в _**-services-namespaces=**_ или env _**SERVICES_NAMESPACE**_), список единиц работы (ключ, по которому хранится такой список, указывается в _**-workunits-namespace=**_ или env _**WORKUNITS_NAMESPACE**_) и так называемую таблицу соответствия (ключ указывается в _**-distribution-namespace=**_ или в env _**DISTRIBUTION_NAMESPACE**_).

Таблица соответствия — это структура данных, в которой каждому сервису сопоставлены некоторые единицы работы. Например, для списка сервисов [service1,service2,service3] и списка единиц работы [workunit1,workunit2,workunit3,workunit4,workunit5,workunit6] таблица соответствия может выглядеть так:

        service1:workunit6,workunit4
        service2:workunit3,workunit1
        service3:workunit2,workunit5
        
Таблица соответствия реализована с помощью [Redis Hash](https://redis.io/topics/data-types).

<br>

#### Распределение работы между сервисами

Для заполнения таблицы соответствия Distributor забирает из Redis список сервисов, список единиц работы и использует консистентное хеширование, чтобы детерминированно сопоставить каждому сервису его единицы работы. 
Для этого мы заносим в hash ring единицы работы (с весом 50 для равномерного распределения), сортируем список сервисов (чтобы порядок занесения сервисов в список не влиял на результат) и затем ищем соответствующее ближайшее значение для каждого сервиса. 
После обнаружения ближайшего значения (единицы работы для данного сервиса), единица работы удаляется с hash ring, чтобы избежать возможных повторных совпадений (когда два сервиса ответственны за одну единицу работы). 

После первоначального распределения может потребоваться перераспределение в случае отказа сервисов, чтобы другие сервисы взяли на себя работу отказавшего. 
Для этого Distributor пингует каждый сервис с заданным интервалом (_**-poll-interval=**_ или env _**POLL_INTERVAL**_) и считает любой неудачный запрос, том числе превысивший таймаут (задается с помощью _**-ping-timeout=**_ или env _**PING_TIMEOUT**_), отказом. 
После обнаружения отказа происходит перераспределение работы по указанному выше алгоритму, после чего новая таблица соответствия заносится в Redis.

<br>

#### Для чего нужно консистентное хеширование

Альтернативна консистентному хешированию — алгоритм, основывающийся на делении с остатком:

        
        key ID mod len(buckets) = i, где
        
        key ID - хеш/контрольная сумма ID сервиса
        len(buckets) - количество единиц работы
        i - индекс конкретной единицы работы в массиве
        
В этом случае изменение количества бакетов (единиц работы в нашем случае) приведет к перераспределению почти всех ключей. 
Консистетное хеширование позволяет избежать излишних перераспределений (**n/m** ключей должны быть перераспределены, где n — количество ключей, а m — количество бакетов) .

https://en.wikipedia.org/wiki/Consistent_hashing

https://web.stanford.edu/class/cs168/l/l1.pdf

<br>

#### Инструментирование сервисов

Каждый сервис под управлением Distributor должен быть доступен для пинга. 
Для этого необходимо внедрить gRPC-сервер в этот сервис:

        import "gitlab.n-t.io/atmz/distributor/pinger/grpc/server"
        ...
        // grpc server for communicating with Distributor
        go server.Inject(opts.conf.Host, opts.conf.Port)
        
Кроме того, каждый сервис при старте должен внести себя в список сервисов в Redis:

        func (r *Redis) RegisterService(namespace, service string) error {
           // check it not registered already
           cmd := r.Client.LRange(namespace, 0, -1)
           var services []string
           if err := cmd.ScanSlice(&services); err != nil {
              return err
           }
           var isRegistered bool
           for _, serviceFound := range services {
              if serviceFound == service {
                 isRegistered = true
              }
           }
           // add to list if not registered already
           if !isRegistered {
              return r.Client.LPush(namespace, service).Err()
           }
           return nil
        }
        
После регистрации в Redis и запуска gRPC сервера сервис будет учитываться в таблице соответствия. Обнаружение изменившегося списка работы для сервиса — это ответственность самого сервиса. 
Для этого ему необходимо периодически проверять Hash (хеш-таблицу) в Redis:

        func (r *Redis) GetMapField(key, field string) ([]string, error) {
           resp := r.Client.HGet(key, field)
           if resp.Err() != nil {
              return nil, resp.Err()
           }
           res, err := resp.Result()
           if err != nil {
              return nil, err
           }
           return strings.Split(res, ","), nil
        }
        
Хотя необходимость проверки таблицы соответствия самим сервисом выглядит избыточной, это снимает нагрузку с Distributor и убирает лишнее звено в сетевых запросах (вместо сервис -> Distributor -> Redis мы ограничиваемся схемой сервис -> Redis). 
К тому же, это снижает риски неконсистентного распределения работы из-за отказов инстансов Distributor и исключает необходимость в решении задачи обнаружения и выбора инстансов Distributor сервисами.

Обновление списка единиц работы в Redis List — это задача отдельного независимого сервиса. 
Ему не нужно никак общаться с другими сервисами, в том числе с Distributor. Этот сервис должен всего лишь в соответствии с конкретной бизнес-логикой добавлять или удалять элементы из Redis List по ключу, который указывается при запуске Distributor в _**-workunits-namespace=**_ или env _**WORKUNITS_NAMESPACE**_. 
Этот сервис невозможно написать заранее, сделать некую дефолтную имплементацию, потому что он целиком и полностью зависит от того, какие единицы работы вы распределяете, откуда и как вы их берете. Впрочем, не обязательно даже писать отдельный сервис, это может быть простым модулем ваших сервисов, для которых создаются распределения — как вам удобно. Всё что вам нужно — обновлять список единиц работы в хранилище.